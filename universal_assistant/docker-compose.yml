services:
  transcriber:
    build:
      context: ./client/transcriber
      dockerfile: Dockerfile
    ports:
      - "7777:7777"
    restart: always
    volumes:
      - whisper_models:/root/.cache/whisper  # Cache Whisper models in default location
      - tts_models:/root/.cache/torch/hub  # Cache NVIDIA TTS models
    environment:
      - PYTHONUNBUFFERED=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:7777/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Give time for Whisper model to download
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  math-server:
    build:
      context: ./servers/math_server
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    restart: always

  weather-server:
    build:
      context: ./servers/weather_server
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    restart: always

  information-server:
    build:
      context: ./servers/process_server
      dockerfile: Dockerfile
    ports:
      - "8004:8004"
    restart: always
    privileged: true

  string-server:
    build:
      context: ./servers/string_server
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    restart: always

volumes:
  whisper_models:  # Named volume for persisting Whisper models
  tts_models:  # Named volume for persisting NVIDIA TTS models

